{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2 — Cleaning & Feature Engineering\n",
        "\n",
        "**Goal:** Turn raw/interim tables into a clean, feature-ready dataset for modeling.\n",
        "\n",
        "**Inputs:** Phase-1 outputs (video + channel tables), utility helpers.\n",
        "\n",
        "**Outputs (example):**\n",
        "- `data/processed/2025-09-18/ml_table.parquet`  (+ CSV if enabled)\n",
        "\n",
        "**Assumptions:**\n",
        "- Timestamps are handled in **UTC**.\n",
        "- Text fields may be empty; features are built to handle missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "3a45d45b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a45d45b",
        "outputId": "efc24d39-3f18-4578-c00c-5a929e428b61"
      },
      "outputs": [],
      "source": [
        "# ===== 1) SETUP =====\n",
        "# --- Standard library ---\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import ast\n",
        "import textwrap\n",
        "import inspect\n",
        "import hashlib\n",
        "import platform\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# --- Third-party ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d0a0f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60d0a0f2",
        "outputId": "b7a77376-e68b-4225-e63a-77405659e9f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===== 2) CONFIG  =====\n",
        "# How Viral is defined: computed an engagement score per video (likes/comments) and then mark the\n",
        "# TOP x% within a *cohort* as positive (viral=1). Everything else is 0.\n",
        "TOP_FRAC   = 0.10      # top 10% of the cohort receive viral=1         \n",
        "\n",
        "COHORT     = \"category_day\"     # \"category_day\" (category × upload date) or \"category\"\n",
        "\n",
        "# Engagement weighting:\n",
        "# Formed a scalar score used for RANKING (not absolute measurement).\n",
        "# score = W_LIKE * likes + W_COMM * comments   (computed within each cohort)\n",
        "W_LIKE     = 0.5                # engagement weight for likes\n",
        "W_COMM     = 0.5                # engagement weight for comments\n",
        "\n",
        "SAVE_CSV   = True              # also write CSV next to the Parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f58dc1e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "f58dc1e4",
        "outputId": "aab03cb3-753f-4133-bdca-8bafabae68d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: /Users/asmitabisht/Desktop/repo\n",
            "CWD set to: /Users/asmitabisht/Desktop/repo\n",
            "Using INPUT_PATH: data/processed/all_days.csv\n"
          ]
        }
      ],
      "source": [
        "# ===== 3) Locating and Loading Dataset  =====\n",
        "def find_repo_root(start: Path = Path.cwd(), must_have=(\"data\",)):\n",
        "    \"\"\"\n",
        "    Walk upward from `start` until we find a directory that contains ALL entries in `must_have`.\n",
        "    Used to robustly locate the repo root (expected to contain 'data/').\n",
        "    \"\"\"\n",
        "    p = start\n",
        "    while True:\n",
        "        if all((p / m).exists() for m in must_have):\n",
        "            return p\n",
        "        if p.parent == p:  # hit filesystem root without success\n",
        "            raise FileNotFoundError(f\"Could not find a repo root above {start} containing: {must_have}\")\n",
        "        p = p.parent\n",
        "\n",
        "# 1) Locate repo root (has a 'data/' folder)\n",
        "REPO_ROOT = find_repo_root(Path.cwd(), must_have=(\"data\",))\n",
        "print(\"Repo root:\", REPO_ROOT)\n",
        "\n",
        "# 2) (Recommended) Switch CWD to repo root so relative paths resolve as 'repo/...'\n",
        "os.chdir(REPO_ROOT)\n",
        "print(\"CWD set to:\", Path.cwd())\n",
        "\n",
        "# 3) Attempt to auto-pick a Phase-2 table from common locations under data/processed/\n",
        "SEARCH_DIRS = [\n",
        "    REPO_ROOT / \"data\" / \"processed\",\n",
        "    REPO_ROOT / \"data\" / \"processed\" / \"phase2_prototype\",\n",
        "]\n",
        "\n",
        "# If preference or an explicit INPUT_PATH earlier\n",
        "PREFERRED_FILE = globals().get(\"PREFERRED_FILE\", None)\n",
        "INPUT_PATH = globals().get(\"INPUT_PATH\", None)\n",
        "\n",
        "# Prefer an explicit filename in data/processed/ if given.\n",
        "if PREFERRED_FILE:\n",
        "    candidate = REPO_ROOT / \"data\" / \"processed\" / PREFERRED_FILE\n",
        "    if candidate.exists():\n",
        "        INPUT_PATH = str(candidate.resolve())\n",
        "\n",
        "# if not pick the newest file that matches the patterns\n",
        "if not INPUT_PATH:\n",
        "    hit = latest_match(PATTERNS, SEARCH_DIRS)\n",
        "    if hit:\n",
        "        INPUT_PATH = str(hit.resolve())\n",
        "\n",
        "print(\"Using INPUT_PATH:\", INPUT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e13cec74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13cec74",
        "outputId": "56734e4f-8cd5-4441-86fe-7a854e268890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 5240 Cols: 20\n",
            "Sample columns: ['approx_age_hours', 'categoryId', 'channelId', 'channel_hiddenSubscriberCount', 'channel_subscriberCount', 'channel_videoCount', 'channel_viewCount', 'comments_72h', 'description', 'duration_sec', 'has_missing_stats', 'is_shorts', 'likes_72h', 'publishedAt_utc', 'snapshot_at_utc', 'source_file', 'tags_str', 'title', 'videoId', 'views_72h']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if p.suffix.lower() == \".parquet\":\n",
        "    df = pd.read_parquet(p)\n",
        "elif p.suffix.lower() == \".csv\":\n",
        "    df = pd.read_csv(p)\n",
        "\n",
        "print(\"Rows:\", len(df), \"Cols:\", len(df.columns))\n",
        "print(\"Sample columns:\", list(df.columns[:25]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f4eb29aa",
      "metadata": {
        "id": "f4eb29aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===== 4) HELPERS  =====\n",
        "\n",
        "# ISO8601 duration parser (PT#H#M#S, P#DT#H#M#S)\n",
        "_ISO_DUR_RE = re.compile(\n",
        "    r'^P(?:(?P<days>\\d+)D)?(?:T(?:(?P<hours>\\d+)H)?(?:(?P<minutes>\\d+)M)?(?:(?P<seconds>\\d+)S)?)?$'\n",
        ")\n",
        "\n",
        "def parse_iso8601_duration_to_seconds(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, (int, float)) and not math.isnan(x):\n",
        "        return float(x)\n",
        "    s = str(x).strip()\n",
        "    m = _ISO_DUR_RE.match(s)\n",
        "    if not m:\n",
        "        return np.nan\n",
        "    d = int(m.group('days') or 0)\n",
        "    h = int(m.group('hours') or 0)\n",
        "    mnt = int(m.group('minutes') or 0)\n",
        "    sec = int(m.group('seconds') or 0)\n",
        "    return float(d*86400 + h*3600 + mnt*60 + sec)\n",
        "\n",
        "def text_is_missing(x: object) -> int:\n",
        "    #1 if title/description is NaN/empty/whitespace, else 0 \n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return 1\n",
        "    return 0 if str(x).strip() else 1\n",
        "\n",
        "# returns the number of whitespaced separated tokens in text\n",
        "def safe_len_words(text):\n",
        "    if pd.isna(text): return 0\n",
        "    return len(str(text).split())\n",
        "\n",
        "# return character length of text after coercion to str\n",
        "def safe_len_chars(text):\n",
        "    if pd.isna(text): return 0\n",
        "    return len(str(text))\n",
        "\n",
        "# return number of upercase letters\n",
        "def caps_ratio(text):\n",
        "    if pd.isna(text): return 0.0\n",
        "    s = re.sub(r'[^A-Za-z]', '', str(text))\n",
        "    if not s: return 0.0\n",
        "    upp = sum(c.isupper() for c in s)\n",
        "    return upp / len(s)\n",
        "\n",
        "#counts occurances of a specific character in text \n",
        "def count_char(text, ch):\n",
        "    if pd.isna(text): return 0\n",
        "    return str(text).count(ch)\n",
        "\n",
        "# calculate tags count\n",
        "def tags_count(x):\n",
        "    if pd.isna(x): return 0\n",
        "    if isinstance(x, (list, tuple)): return len(x)\n",
        "    s = str(x)\n",
        "    # Try JSON-like list\n",
        "    if s.startswith('[') and s.endswith(']'):\n",
        "        try:\n",
        "            import ast\n",
        "            lst = ast.literal_eval(s)\n",
        "            return len(lst) if isinstance(lst, list) else 0\n",
        "        except Exception:\n",
        "            pass\n",
        "    # comma separated\n",
        "    if ',' in s: return len([t for t in s.split(',') if t.strip()])\n",
        "    return 1 if s.strip() else 0\n",
        "\n",
        "# Turn tags into a space-separated 'bag of tags' string. Missing -> '' (empty string), which is safe for TF-IDF.\n",
        "def tags_to_text(x: object) -> str:\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return \"\"\n",
        "    toks = []\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        toks = [str(t).strip().lower() for t in x if str(t).strip()]\n",
        "    else:\n",
        "        s = str(x)\n",
        "        if s.startswith('[') and s.endswith(']'):\n",
        "            try:\n",
        "                lst = ast.literal_eval(s)\n",
        "                toks = [str(t).strip().lower() for t in lst if str(t).strip()]\n",
        "            except Exception:\n",
        "                toks = [t.strip().lower() for t in re.split(r'[|,]', s) if t.strip()]\n",
        "        else:\n",
        "            toks = [t.strip().lower() for t in re.split(r'[|,]', s) if t.strip()]\n",
        "    # keep multi-word tags as a single token\n",
        "    toks = [t.replace(' ', '_') for t in toks]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "def to_utc_ts(x):\n",
        "    # parses ISO timestamps, returns timezone-aware pandas Timestamp or NaT\n",
        "    return pd.to_datetime(x, utc=True, errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd0123f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd0123f",
        "outputId": "a921f38b-24f7-4274-b9c9-94e119b9467d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Engineered feature columns now present:\n",
            "['pub_hour', 'pub_dow', 'pub_hour_sin', 'pub_hour_cos', 'title_len_words', 'title_len_chars', 'title_caps_ratio', 'title_num_exclaim', 'title_num_question', 'desc_len_words', 'title_missing', 'title_has_question', 'title_has_exclaim', 'title_emoji_count', 'desc_emoji_count', 'tags_count', 'has_tags', 'channel_subscriberCount_log1p', 'channel_viewCount_log1p', 'channel_videoCount_log1p', 'duration_log1p', 'engagement_score']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 5) FEATURE ENGINEERING  =====\n",
        "\n",
        "df_feat = df.copy()\n",
        "\n",
        "# ensure expected non-tag columns exist\n",
        "for col in [\n",
        "    'title','description','categoryId','publishedAt_utc','duration','duration_sec',\n",
        "    'channel_subscriberCount','channel_viewCount','channel_videoCount',\n",
        "    'likes_72h','comments_72h'\n",
        "]:\n",
        "    if col not in df_feat.columns:\n",
        "        df_feat[col] = np.nan\n",
        "\n",
        "# unify raw tags into df_feat['tags'] ===\n",
        "if 'tags' not in df_feat.columns:\n",
        "    if 'tags_str' in df_feat.columns:\n",
        "        df_feat['tags'] = df_feat['tags_str']          # comma/whitespace-separated\n",
        "    elif 'tags_json' in df_feat.columns:\n",
        "        df_feat['tags'] = df_feat['tags_json']         # JSON list as string\n",
        "    else:\n",
        "        df_feat['tags'] = np.nan                       # truly missing\n",
        "\n",
        "\n",
        "# publish time buckets\n",
        "ts = df_feat['publishedAt_utc'].apply(to_utc_ts)\n",
        "df_feat['pub_hour']   = ts.dt.hour.astype('float')\n",
        "df_feat['pub_dow']    = ts.dt.weekday.astype('float')\n",
        "df_feat['upload_day'] = ts.dt.date.astype('string')  # for category × day cohorting\n",
        "\n",
        "# cyclical time features (works better than raw hour)\n",
        "import numpy as np\n",
        "df_feat['pub_hour_sin'] = np.sin(2*np.pi*df_feat['pub_hour']/24)\n",
        "df_feat['pub_hour_cos'] = np.cos(2*np.pi*df_feat['pub_hour']/24)\n",
        "\n",
        "# duration (seconds) — if missing, parse ISO 8601 string\n",
        "if 'duration_sec' not in df_feat.columns or df_feat['duration_sec'].isna().all():\n",
        "    df_feat['duration_sec'] = df_feat['duration'].apply(parse_iso8601_duration_to_seconds)\n",
        "\n",
        "# text features (title/description); intentionally simple fast heuristics\n",
        "df_feat['title_len_words']    = df_feat['title'].apply(safe_len_words)\n",
        "df_feat['title_len_chars']    = df_feat['title'].apply(safe_len_chars)\n",
        "df_feat['title_caps_ratio']   = df_feat['title'].apply(caps_ratio)\n",
        "df_feat['title_num_exclaim']  = df_feat['title'].apply(lambda s: count_char(s,'!'))\n",
        "df_feat['title_num_question'] = df_feat['title'].apply(lambda s: count_char(s,'?'))\n",
        "df_feat['desc_len_words']     = df_feat['description'].apply(safe_len_words)\n",
        "\n",
        "# missingness indicators\n",
        "df_feat['title_missing']       = df_feat['title'].apply(text_is_missing).astype(int)\n",
        "df_feat['description_missing'] = df_feat['description'].apply(text_is_missing).astype(int)\n",
        "\n",
        "# binary features and emoji counts\n",
        "df_feat['title_has_question'] = df_feat['title'].str.contains(r'\\?').fillna(False).astype(int)\n",
        "df_feat['title_has_exclaim']  = df_feat['title'].str.contains(r'!').fillna(False).astype(int)\n",
        "df_feat['title_emoji_count']  = df_feat['title'].str.count(r'[\\U00010000-\\U0010ffff]').fillna(0)\n",
        "df_feat['desc_emoji_count']   = df_feat['description'].str.count(r'[\\U00010000-\\U0010ffff]').fillna(0)\n",
        "\n",
        "# tags\n",
        "df_feat['tags_count'] = df_feat['tags'].apply(tags_count)\n",
        "df_feat['has_tags']   = (df_feat['tags_count'] > 0).astype(int)\n",
        "df_feat['tags_text']  = df_feat['tags'].apply(tags_to_text)\n",
        "\n",
        "\n",
        "# channel scale features \n",
        "for c in ['channel_subscriberCount','channel_viewCount','channel_videoCount']:\n",
        "    lc = c + '_log1p'\n",
        "    df_feat[lc] = np.log1p(pd.to_numeric(df_feat[c], errors='coerce'))\n",
        "\n",
        "# duration log\n",
        "df_feat['duration_log1p'] = np.log1p(pd.to_numeric(df_feat['duration_sec'], errors='coerce'))\n",
        "\n",
        "# engagement score \n",
        "likes = pd.to_numeric(df_feat['likes_72h'], errors='coerce').fillna(0)\n",
        "comms = pd.to_numeric(df_feat['comments_72h'], errors='coerce').fillna(0)\n",
        "df_feat['engagement_score'] = W_LIKE*np.log1p(likes) + W_COMM*np.log1p(comms)\n",
        "\n",
        "print(\"Engineered feature columns now present:\")\n",
        "print([c for c in df_feat.columns\n",
        "       if c.endswith('_log1p')\n",
        "       or c.startswith(('title_','desc_','tags_count','pub_','has_tags','engagement_score'))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b1fcc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73b1fcc6",
        "outputId": "963752f7-92ab-4714-bd35-8fbf5feea2c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label prevalence (overall): 0.132\n",
            "Per-category sample prevalence:\n",
            "categoryId\n",
            "1     0.171\n",
            "2     0.308\n",
            "10    0.167\n",
            "15    0.571\n",
            "17    0.115\n",
            "19    0.536\n",
            "20    0.110\n",
            "22    0.118\n",
            "23    0.179\n",
            "24    0.111\n",
            "Name: viral, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# ===== 6) COHORT PERCENTILES & LABELS  =====\n",
        "# cohort for percentile: category × upload_day (or just category)\n",
        "if COHORT == \"category_day\" and 'upload_day' in df_feat.columns:\n",
        "    grp = df_feat.groupby(['categoryId','upload_day'], dropna=False)['engagement_score']\n",
        "else:\n",
        "    grp = df_feat.groupby(['categoryId'], dropna=False)['engagement_score']\n",
        "\n",
        "# percentile within cohort and top-frac label\n",
        "df_feat['engagement_percentile'] = grp.rank(pct=True, method='average')\n",
        "df_feat['viral'] = (df_feat['engagement_percentile'] >= (1.0 - TOP_FRAC)).astype(int)\n",
        "\n",
        "print(\"Label prevalence (overall):\", df_feat['viral'].mean().round(3))\n",
        "try:\n",
        "    print(\"Per-category sample prevalence:\")\n",
        "    print(df_feat.groupby('categoryId')['viral'].mean().round(3).head(10))\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "31a40887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31a40887",
        "outputId": "0c823f39-e2c1-40a1-f773-64a100e7281b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in output: 35\n",
            "['videoId', 'channelId', 'categoryId', 'publishedAt_utc', 'upload_day', 'duration_log1p', 'tags_count', 'title_len_words', 'title_len_chars', 'title_caps_ratio', 'title_num_exclaim', 'title_num_question', 'desc_len_words', 'pub_hour', 'pub_dow', 'channel_subscriberCount_log1p', 'channel_viewCount_log1p', 'channel_videoCount_log1p', 'title_has_question', 'title_has_exclaim', 'title_emoji_count', 'desc_emoji_count', 'pub_hour_sin', 'pub_hour_cos', 'has_tags', 'title_missing', 'description_missing', 'title', 'description', 'tags_text', 'likes_72h', 'comments_72h', 'engagement_score', 'engagement_percentile', 'viral']\n",
            "Rows: 5240 Viral rate: 0.132\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 7) SELECT OUTPUT COLUMNS  =====\n",
        "\n",
        "# selecting what is kept: core IDs + features + raw text + engagement fields + label\n",
        "keep = [\n",
        "  # ids/meta\n",
        "    'videoId','channelId','categoryId','publishedAt_utc','upload_day',\n",
        "    # numeric features\n",
        "    'duration_log1p','tags_count','title_len_words','title_len_chars','title_caps_ratio',\n",
        "    'title_num_exclaim','title_num_question','desc_len_words','pub_hour','pub_dow',\n",
        "    'channel_subscriberCount_log1p','channel_viewCount_log1p','channel_videoCount_log1p', 'title_has_question','title_has_exclaim',\n",
        "    'title_emoji_count','desc_emoji_count',\n",
        "    'pub_hour_sin','pub_hour_cos',\n",
        "    # binary “presence/missing”\n",
        "    'has_tags','title_missing','description_missing',\n",
        "    # text passthrough\n",
        "    'title','description','tags_text',\n",
        "    # engagement + label\n",
        "    'likes_72h','comments_72h','engagement_score','engagement_percentile','viral'\n",
        "]\n",
        "\n",
        "exists = [c for c in keep if c in df_feat.columns]\n",
        "df_out = df_feat[exists].copy()\n",
        "\n",
        "print(\"Columns in output:\", len(exists))\n",
        "print(exists)\n",
        "print(\"Rows:\", len(df_out), \"Viral rate:\", df_out['viral'].mean().round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58642d18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "58642d18",
        "outputId": "22a446d2-d3e0-479a-a510-239c244e5a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: /Users/asmitabisht/Desktop/repo\n",
            "Saving under: /Users/asmitabisht/Desktop/repo/data/processed\n",
            "Saved: /Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z.parquet\n",
            "Saved: /Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z.csv\n",
            "Saved: /Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z_summary.json\n",
            "{\n",
            "  \"generated_utc\": \"2025-09-22T03-12-02Z\",\n",
            "  \"input_path\": \"data/processed/all_days.csv\",\n",
            "  \"rows\": 5240,\n",
            "  \"viral_rate\": 0.1316793893129771,\n",
            "  \"cohort\": \"category_day\",\n",
            "  \"top_frac\": 0.1,\n",
            "  \"weights\": {\n",
            "    \"likes\": 0.5,\n",
            "    \"comments\": 0.5\n",
            "  },\n",
            "  \"columns\": [\n",
            "    \"videoId\",\n",
            "    \"channelId\",\n",
            "    \"categoryId\",\n",
            "    \"publishedAt_utc\",\n",
            "    \"upload_day\",\n",
            "    \"duration_log1p\",\n",
            "    \"tags_count\",\n",
            "    \"title_len_words\",\n",
            "    \"title_len_chars\",\n",
            "    \"title_caps_ratio\",\n",
            "    \"title_num_exclaim\",\n",
            "    \"title_num_question\",\n",
            "    \"desc_len_words\",\n",
            "    \"pub_hour\",\n",
            "    \"pub_dow\",\n",
            "    \"channel_subscriberCount_log1p\",\n",
            "    \"channel_viewCount_log1p\",\n",
            "    \"channel_videoCount_log1p\",\n",
            "    \"title_has_question\",\n",
            "    \"title_has_exclaim\",\n",
            "    \"title_emoji_count\",\n",
            "    \"desc_emoji_count\",\n",
            "    \"pub_hour_sin\",\n",
            "    \"pub_hour_cos\",\n",
            "    \"has_tags\",\n",
            "    \"title_missing\",\n",
            "    \"description_missing\",\n",
            "    \"title\",\n",
            "    \"description\",\n",
            "    \"tags_text\",\n",
            "    \"likes_72h\",\n",
            "    \"comments_72h\",\n",
            "    \"engagement_score\",\n",
            "    \"engagement_percentile\",\n",
            "    \"viral\"\n",
            "  ],\n",
            "  \"output_parquet\": \"/Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z.parquet\",\n",
            "  \"output_csv\": \"/Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z.csv\"\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"margin:8px 0;padding:10px 12px;border-left:4px solid #4a90e2;background:#f7f9fc\">\n",
              "  <b>Artifacts saved to:</b> <code>/Users/asmitabisht/Desktop/repo/data/processed</code>\n",
              "  <ul style=\"margin:6px 0 0 18px\"><li><a href=\"/Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z.parquet\">ml_table_2025-09-22T03-12-02Z.parquet</a></li><li><a href=\"/Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z.csv\">ml_table_2025-09-22T03-12-02Z.csv</a></li><li><a href=\"/Users/asmitabisht/Desktop/repo/data/processed/ml_table_2025-09-22T03-12-02Z_summary.json\">ml_table_2025-09-22T03-12-02Z_summary.json</a></li></ul>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ===== 8) SAVE OUTPUTS  =====\n",
        "\n",
        "# ---- Config \n",
        "SAVE_PARQUET = True\n",
        "SAVE_CSV     = True                 # write CSV alongside Parquet\n",
        "CSV_ENCODING = \"utf-8\"              # use 'utf-8-sig' if opening with old Excel (for testing)\n",
        "CSV_INDEX    = False\n",
        "PARQUET_COMPRESSION = \"snappy\"    \n",
        "\n",
        "# ---- find repo root so its always saved to repo/data/processed\n",
        "def _find_repo_root(start: Path = Path.cwd(), must_have=(\"data\",)) -> Path:\n",
        "    p = start\n",
        "    while True:\n",
        "        if all((p / m).exists() for m in must_have):\n",
        "            return p\n",
        "        if p.parent == p:\n",
        "            # Fallback: create data/ in the current dir if not found\n",
        "            (start / \"data\").mkdir(parents=True, exist_ok=True)\n",
        "            return start\n",
        "        p = p.parent\n",
        "\n",
        "REPO_ROOT = _find_repo_root()\n",
        "PROCESSED_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#  optional for google collab testing\n",
        "print(\"Repo root:\", REPO_ROOT.as_posix())\n",
        "print(\"Saving under:\", PROCESSED_DIR.as_posix())\n",
        "\n",
        "# ---- filenames\n",
        "stamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n",
        "base_name = f\"ml_table_{stamp}\"\n",
        "out_pq  = PROCESSED_DIR / f\"{base_name}.parquet\"\n",
        "out_csv = PROCESSED_DIR / f\"{base_name}.csv\"\n",
        "sum_js  = PROCESSED_DIR / f\"{base_name}_summary.json\"\n",
        "\n",
        "# ---- write parquet / CSV\n",
        "if SAVE_PARQUET:\n",
        "    df_out.to_parquet(out_pq, index=False, compression=PARQUET_COMPRESSION)\n",
        "\n",
        "if SAVE_CSV:\n",
        "    df_out.to_csv(out_csv, index=CSV_INDEX, encoding=CSV_ENCODING)\n",
        "\n",
        "# ---- summary JSON \n",
        "summary = {\n",
        "    \"generated_utc\": stamp,\n",
        "    \"input_path\": str(globals().get(\"INPUT_PATH\", \"\")),\n",
        "    \"rows\": int(len(df_out)),\n",
        "    \"viral_rate\": float(df_out[\"viral\"].mean()) if \"viral\" in df_out.columns else None,\n",
        "    \"cohort\": globals().get(\"COHORT\", None),\n",
        "    \"top_frac\": globals().get(\"TOP_FRAC\", None),\n",
        "    \"weights\": {\n",
        "        \"likes\":  globals().get(\"W_LIKE\", None),\n",
        "        \"comments\": globals().get(\"W_COMM\", None)\n",
        "    },\n",
        "    \"columns\": list(df_out.columns),\n",
        "    \"output_parquet\": out_pq.as_posix() if SAVE_PARQUET else None,\n",
        "    \"output_csv\": out_csv.as_posix() if SAVE_CSV else None\n",
        "}\n",
        "with open(sum_js, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# ---- console prints\n",
        "print(\"Saved:\", out_pq.as_posix() if SAVE_PARQUET else \"(Parquet disabled)\")\n",
        "print(\"Saved:\", out_csv.as_posix() if SAVE_CSV else \"(CSV disabled)\")\n",
        "print(\"Saved:\", sum_js.as_posix())\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# ---- notebook links (nice to click)\n",
        "links = []\n",
        "if SAVE_PARQUET: links.append(f'<li><a href=\"{out_pq.as_posix()}\">{out_pq.name}</a></li>')\n",
        "if SAVE_CSV:     links.append(f'<li><a href=\"{out_csv.as_posix()}\">{out_csv.name}</a></li>')\n",
        "links.append(f'<li><a href=\"{sum_js.as_posix()}\">{sum_js.name}</a></li>')\n",
        "display(HTML(f\"\"\"\n",
        "<div style=\"margin:8px 0;padding:10px 12px;border-left:4px solid #4a90e2;background:#f7f9fc\">\n",
        "  <b>Artifacts saved to:</b> <code>{PROCESSED_DIR.as_posix()}</code>\n",
        "  <ul style=\"margin:6px 0 0 18px\">{''.join(links)}</ul>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "# ---- Optional: auto-download in Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        if SAVE_CSV and out_csv.exists():\n",
        "            files.download(out_csv.as_posix())\n",
        "        elif SAVE_PARQUET and out_pq.exists():\n",
        "            files.download(out_pq.as_posix())\n",
        "    except Exception as e:\n",
        "        print(\"Colab download skipped:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
