"""
test_features.py

This test is a Sanity-check for the feature engineering pipeline behavior and determinism. 

"""

import numpy as np
import pandas as pd
import pytest
from math import ceil
from sklearn.base import clone
from sklearn.feature_extraction.text import TfidfVectorizer

def _max_min_df_in_pipeline(pipe) -> int:
    """
    Walk the ColumnTransformer / Pipelines to find the largest *integer* min_df
    among any TfidfVectorizer. If min_df is a float, we don't need to bump rows.
    """
    max_min_df = 0
    if hasattr(pipe, "transformers"):
        iters = [(None, t, cols) for (name, t, cols) in pipe.transformers]
    else:
        iters = []
    for _, trans, cols in iters:
        # direct vectorizer
        if isinstance(trans, TfidfVectorizer):
            md = trans.min_df
            if isinstance(md, int):
                max_min_df = max(max_min_df, md)
        # nested pipeline
        if hasattr(trans, "steps"):
            for _, step in trans.steps:
                if isinstance(step, TfidfVectorizer):
                    md = step.min_df
                    if isinstance(md, int):
                        max_min_df = max(max_min_df, md)
    return max_min_df

def _pad_and_enrich_text(df: pd.DataFrame, need_rows: int, text_cols):
    """Pad to `need_rows` and insert non-stopword tokens to satisfy min_df."""
    df2 = df.copy()
    pad = max(0, need_rows - len(df2))
    if pad > 0:
        pad_block = pd.concat([df2.iloc[:1].copy() for _ in range(pad)], ignore_index=True)
        df2 = pd.concat([df2.reset_index(drop=True), pad_block], ignore_index=True)
    for c in text_cols:
        if c in df2.columns:
            s = df2[c].astype(str).fillna("")
            # avoid pure stopwords; add repeated tokens to hit min_df
            s = np.where(pd.Series(s).str.len() < 2, "video data", s)
            s = s + " model"
            s = np.where((np.arange(len(s)) % 2) == 0, s + " youtube", s)
            df2[c] = s
    return df2

def _safe_fit(pipe, df):
    """
    Fit the pipeline. If a TfidfVectorizer has integer min_df > n_rows,
    pad/enrich the text columns so vocabulary can be built.
    """
    need_rows = max(6, _max_min_df_in_pipeline(pipe) + 1)  # +1 for cushion
    text_cols = [c for c in df.columns if c.lower() in ("title", "description")]
    df2 = _pad_and_enrich_text(df, need_rows, text_cols)
    return pipe.fit(df2)

@pytest.mark.skipif(True, reason="Replaced by pipeline-determinism test below if pipeline exists")
def test_features_identity_placeholder():
    # Kept only to avoid accidental empty file; not used.
    pass

def test_features_deterministic(tiny_raw_df, feature_pipeline):
    """
    Determinism check: fit your ColumnTransformer once, then transform twice -> identical.
    Skips if you haven't exported a preprocessing pipeline artifact.
    """
    if feature_pipeline is None:
        pytest.skip("No sklearn feature pipeline exported (models/preprocess_phase3.joblib)")

    pipe = clone(feature_pipeline)
    _safe_fit(pipe, tiny_raw_df)

    X1 = pipe.transform(tiny_raw_df)
    X2 = pipe.transform(tiny_raw_df)

    # Convert to dense arrays if needed, then compare element-wise
    if hasattr(X1, "toarray"): X1 = X1.toarray()
    if hasattr(X2, "toarray"): X2 = X2.toarray()
    assert X1.shape == X2.shape
    assert np.allclose(X1, X2, atol=0, rtol=0), "Feature pipeline not deterministic on identical input"

def test_handles_edge_cases():
    # Generic featurizer smoke; doesn't depend on your real pipeline
    df = pd.DataFrame({
        "a":[0, None, 1e12, -5],
        "b":["", None, "ðŸ™‚"*1000, "ok"],
        "c":[True, False, True, False],
    })
    assert len(df) == 4
    assert isinstance(df, pd.DataFrame)
