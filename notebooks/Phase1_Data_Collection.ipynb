{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "QTN236gwyH85",
      "metadata": {
        "id": "QTN236gwyH85"
      },
      "source": [
        "# Phase 1 — Data Collection\n",
        "\n",
        "**Goal:** Collect YouTube video metadata for a target UTC calendar day, with a widened publish window to improve recall.\n",
        "\n",
        "**Inputs:** YouTube Data API v3 (search, videos, channels)\n",
        "\n",
        "**Outputs (example):**\n",
        "- `data/raw/2025-09-18/search/*.json` (optional raw snapshots)\n",
        "- `data/processed/2025-09-18/videos.parquet`  (+ CSV if enabled)\n",
        "- `data/processed/2025-09-18/channels.parquet` (optional)\n",
        "\n",
        "**Assumptions:**\n",
        "- Timestamps are treated as **UTC**.\n",
        "- API key is provided via environment variable `YOUTUBE_API_KEY` (or `.env`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "helpers",
      "metadata": {
        "id": "helpers"
      },
      "outputs": [],
      "source": [
        "# ===== 1) SETUP =====\n",
        "# --- Standard library ---\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# --- Third-party ---\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv, find_dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "config",
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# ===== 2) CONFIG =====\n",
        "os.environ.pop(\"YOUTUBE_API_KEY\", None)                 \n",
        "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
        "\n",
        "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
        "assert API_KEY and API_KEY.strip(), \"Missing YOUTUBE_API_KEY in environment or .env\"\n",
        "# Filtering strategy: relaxed, not strict. In practice, tight constraints (e.g., extremely narrow publish windows + strictly searching english)\n",
        "# collapsed daily results to near zero for several categories/regions. Prioritizing recall to ensure enough data and clean downstream\n",
        "\n",
        "REGION_CODE = \"US\"          # only search videos from US to bias toward English titles\n",
        "RELEVANCE_LANGUAGE = None   # hint for search (not a strict filter so still possible to get non-eng titles)\n",
        "DAYS_AGO = 3                 # target the UTC calendar day N days ago\n",
        "WIDEN_HOURS = 12             # Hours widen the target day (broadening publish window looked at)\n",
        "SEARCH_UNITS = 9000          # budget for search.list (100 units per page)\n",
        "SAVE_CSV = True              # also write CSV alongside Parquet\n",
        "SAVE_RAW = True              # save raw JSON responses (good for debugging)\n",
        "OUT_ROOT = \"data\"           # output root folder (created if missing)\n",
        "\n",
        "# YouTube API base\n",
        "YOUTUBE_API_BASE = \"https://www.googleapis.com/youtube/v3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7230f735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== 3) HELPERS =====\n",
        "\n",
        "# Creates a directory if it doesnt exist\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# Current UTC timestamp in ISO-like format (YYYY-mm-ddTHH:MM:SSZ).\n",
        "def ts_now_utc() -> str:\n",
        "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n",
        "\n",
        "#   Returns (publishedAfter, publishedBefore) formatted as RFC3339 for the UTC day 'days_ago' before now.\n",
        "def iso_day_bounds_utc(days_ago: int):\n",
        "    now = datetime.now(timezone.utc)\n",
        "    target_day = (now - timedelta(days=days_ago)).date()\n",
        "    start = datetime(target_day.year, target_day.month, target_day.day, 0, 0, 0, tzinfo=timezone.utc)\n",
        "    end   = datetime(target_day.year, target_day.month, target_day.day, 23, 59, 59, tzinfo=timezone.utc)\n",
        "    return start.isoformat().replace(\"+00:00\",\"Z\"), end.isoformat().replace(\"+00:00\",\"Z\")\n",
        "\n",
        "# Convert an ISO-8601 duration from the API (e.g., 'PT2H3M10S') into seconds.\n",
        "# - Handles hours/minutes/seconds parts (PT#H#M#S). Ignores weeks/months/years.\n",
        "# - Returns None if the input is empty or not a 'PT...' duration.\n",
        "def parse_iso8601_duration_to_seconds(iso_dur: str):\n",
        "    if not iso_dur or not iso_dur.startswith(\"PT\"):\n",
        "        return None\n",
        "    total = 0\n",
        "    num = ''\n",
        "    for ch in iso_dur[2:]:\n",
        "        if ch.isdigit():\n",
        "            num += ch\n",
        "        else:\n",
        "            if ch == 'H' and num:\n",
        "                total += int(num) * 3600\n",
        "            elif ch == 'M' and num:\n",
        "                total += int(num) * 60\n",
        "            elif ch == 'S' and num:\n",
        "                total += int(num)\n",
        "            num = ''\n",
        "    return total\n",
        "\n",
        "# Conversion to int, returns none if fails \n",
        "def safe_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Yields successive chunks of size \"n\" from list \"lst\" (for batching API calls)\n",
        "def chunked(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "# Thin wrapper around GET requests to YouTube Data API v3.\n",
        "#   - Appends the API key to the query string\n",
        "#   - Raises for HTTP errors \n",
        "#   - Returns the parsed JSON response\n",
        "#   - Uses a 30s timeout to avoid hanging forever\n",
        "def yt_get(endpoint: str, params: Dict[str, Any], api_key: str) -> Dict[str, Any]:\n",
        "    url = f\"{YOUTUBE_API_BASE}/{endpoint}\"\n",
        "    p = params.copy()\n",
        "    p[\"key\"] = api_key\n",
        "    r = requests.get(url, params=p, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "api_wrappers",
      "metadata": {
        "id": "api_wrappers"
      },
      "outputs": [],
      "source": [
        "# ===== 4) API WRAPPERS  =====\n",
        "\n",
        "# Fetches YouTube video categories for a given region\n",
        "def fetch_categories(api_key: str, region_code: str, raw_dir: str):\n",
        "    params = {\"part\": \"snippet\", \"regionCode\": region_code, \"hl\": \"en_US\"}\n",
        "    data = yt_get(\"videoCategories\", params, api_key)\n",
        "    # Optional raw capture for reproducibility and auditing \n",
        "    if SAVE_RAW:\n",
        "        raw_path = os.path.join(raw_dir, f\"categories_{region_code}_{ts_now_utc()}.json\")\n",
        "        with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    # Flattens the response into rows\n",
        "    rows = []\n",
        "    for item in data.get(\"items\", []):\n",
        "        rows.append({\n",
        "            \"id\": item.get(\"id\"),\n",
        "            \"title\": item.get(\"snippet\", {}).get(\"title\"),\n",
        "            \"assignable\": item.get(\"snippet\", {}).get(\"assignable\", False),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# performs a single pass of pagniated search requests \n",
        "def _search_pass(api_key, params_base, pages, raw_dir, tag):\n",
        "    ids = []\n",
        "    next_page = None\n",
        "    for i in range(1, pages+1):\n",
        "        p = dict(params_base)\n",
        "        if next_page:\n",
        "            p[\"pageToken\"] = next_page\n",
        "        data = yt_get(\"search\", p, api_key)\n",
        "        # Optional raw capture of each page \n",
        "        if SAVE_RAW:\n",
        "            with open(os.path.join(raw_dir, f\"search_{tag}_p{i:03d}_{ts_now_utc()}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        # Extract VideoIds only\n",
        "        ids.extend([it.get(\"id\", {}).get(\"videoId\") for it in data.get(\"items\", []) if it.get(\"id\", {}).get(\"videoId\")])\n",
        "        # Handles pagination\n",
        "        next_page = data.get(\"nextPageToken\")\n",
        "        if not next_page:\n",
        "            break\n",
        "        # Small pause to be polite and avoid triggering rate limits\n",
        "        time.sleep(0.15)\n",
        "    return ids\n",
        "\n",
        "# Broad search sweep for videos in a UTC day window, with three robustness passes:\n",
        "def search_all_categories_day(api_key: str, region_code: str, relevance_language: str,\n",
        "                              published_after: str, published_before: str,\n",
        "                              search_units_budget: int, raw_dir: str,\n",
        "                              page_pause_s: float = 0.15):\n",
        "    # convert budget (units) to pages (100 units per page)\n",
        "    total_pages = max(1, search_units_budget // 100)\n",
        "    collected = set()\n",
        "\n",
        "    # Base params (no q)\n",
        "    base = {\n",
        "        \"part\": \"id\",\n",
        "        \"type\": \"video\",\n",
        "        \"regionCode\": region_code,\n",
        "        \"publishedAfter\": published_after,\n",
        "        \"publishedBefore\": published_before,\n",
        "        \"order\": \"date\",\n",
        "        \"maxResults\": 50,\n",
        "    }\n",
        "    if relevance_language:  # only add if set\n",
        "        base[\"relevanceLanguage\"] = relevance_language\n",
        "\n",
        "    # PASS 1 — no q\n",
        "    ids = _search_pass(api_key, base, pages=min(10, total_pages//3 or 1), raw_dir=raw_dir, tag=\"p1_allcats_noq\")\n",
        "    collected.update([i for i in ids if i])\n",
        "\n",
        "    # PASS 2 — light q-seeds to force results if pass 1 was thin\n",
        "    if len(collected) < 200:\n",
        "        q_seeds = [\"a\",\"e\",\"i\",\"o\",\"u\",\"the\",\"and\",\"1\",\"2\",\"3\"]\n",
        "        pages_left = max(1, total_pages//3)\n",
        "        per_seed = max(1, pages_left // len(q_seeds))\n",
        "        for q in q_seeds:\n",
        "            b = dict(base); b[\"q\"] = q\n",
        "            ids = _search_pass(api_key, b, pages=per_seed, raw_dir=raw_dir, tag=f\"p2_seed_{q}\")\n",
        "            collected.update([i for i in ids if i])\n",
        "\n",
        "    # PASS 3 — per-category fallback if still thin\n",
        "    if len(collected) < 200:\n",
        "        try:\n",
        "            cats = fetch_categories(api_key, region_code, raw_dir)\n",
        "            cat_ids = [c for c in cats.loc[cats.assignable==True, \"id\"].tolist() if c]\n",
        "        except Exception:\n",
        "            cat_ids = [\"10\",\"24\",\"20\",\"27\",\"28\",\"17\",\"25\",\"26\",\"22\"]  # common categories fallback\n",
        "        pages_left = max(1, total_pages - (len(collected)//50) - 10)\n",
        "        per_cat = max(1, pages_left // max(1,len(cat_ids)))\n",
        "        for cid in cat_ids:\n",
        "            b = dict(base); b[\"videoCategoryId\"] = cid\n",
        "            ids = _search_pass(api_key, b, pages=per_cat, raw_dir=raw_dir, tag=f\"p3_cat_{cid}\")\n",
        "            collected.update([i for i in ids if i])\n",
        "\n",
        "    return list(collected)\n",
        "\n",
        "#    Fetch details/statistics for a list of video IDs via videos.list.\n",
        "def videos_details(api_key: str, video_ids: List[str], raw_dir: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    batches = list(chunked(video_ids, 50))\n",
        "    for i, batch in enumerate(tqdm(batches, desc=\"videos.list batches\", unit=\"batch\"), start=1):\n",
        "        params = {\"part\": \"snippet,contentDetails,statistics\", \"id\": \",\".join(batch)}\n",
        "        data = yt_get(\"videos\", params, api_key)\n",
        "         # Optional raw capture per batch\n",
        "        if SAVE_RAW:\n",
        "            raw_path = os.path.join(raw_dir, f\"videos_batch{i:04d}_{ts_now_utc()}.json\")\n",
        "            with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        for it in data.get(\"items\", []):\n",
        "            sn = it.get(\"snippet\", {}) or {}\n",
        "            st = it.get(\"statistics\", {}) or {}\n",
        "            cd = it.get(\"contentDetails\", {}) or {}\n",
        "            duration_sec = parse_iso8601_duration_to_seconds(cd.get(\"duration\"))\n",
        "            tags = sn.get(\"tags\") or []\n",
        "            rows.append({\n",
        "                \"videoId\": it.get(\"id\"),\n",
        "                \"channelId\": sn.get(\"channelId\"),\n",
        "                \"categoryId\": sn.get(\"categoryId\"),\n",
        "                \"publishedAt\": sn.get(\"publishedAt\"),\n",
        "                \"title\": sn.get(\"title\"),\n",
        "                \"description\": sn.get(\"description\"),\n",
        "                \"tags_str\": \",\".join(tags) if isinstance(tags, list) else None,\n",
        "                \"duration_sec\": duration_sec,\n",
        "                \"viewCount\": safe_int(st.get(\"viewCount\")),\n",
        "                \"likeCount\": safe_int(st.get(\"likeCount\")),\n",
        "                \"commentCount\": safe_int(st.get(\"commentCount\")),\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "#Fetch channel-level statistics (viewCount, subscriberCount, videoCount, hiddenSubscriberCount).\n",
        "def channels_stats(api_key: str, channel_ids: List[str], raw_dir: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    seen = set()\n",
        "    # Deduplicate, preserve order\n",
        "    uniq = [c for c in channel_ids if c and c not in seen and not seen.add(c)]\n",
        "    batches = list(chunked(uniq, 50))\n",
        "    for i, batch in enumerate(tqdm(batches, desc=\"channels.list batches\", unit=\"batch\"), start=1):\n",
        "        params = {\"part\": \"statistics\", \"id\": \",\".join(batch)}\n",
        "        data = yt_get(\"channels\", params, api_key)\n",
        "        # Optional raw capture per batch\n",
        "        if SAVE_RAW:\n",
        "            raw_path = os.path.join(raw_dir, f\"channels_batch{i:04d}_{ts_now_utc()}.json\")\n",
        "            with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        for it in data.get(\"items\", []):\n",
        "            st = it.get(\"statistics\", {}) or {}\n",
        "            rows.append({\n",
        "                \"channelId\": it.get(\"id\"),\n",
        "                \"channel_viewCount\": safe_int(st.get(\"viewCount\")),\n",
        "                \"channel_subscriberCount\": safe_int(st.get(\"subscriberCount\")),\n",
        "                \"channel_videoCount\": safe_int(st.get(\"videoCount\")),\n",
        "                \"channel_hiddenSubscriberCount\": bool(st.get(\"hiddenSubscriberCount\")) if st.get(\"hiddenSubscriberCount\") is not None else None\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_pipeline",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run_pipeline",
        "outputId": "458de88f-b6ae-4af8-d357-5e2fd0989950"
      },
      "outputs": [],
      "source": [
        "# ===== 5) RUN COLLECTION  =====\n",
        "\n",
        "raw_day = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "raw_dir = os.path.join(OUT_ROOT, \"raw\", raw_day)\n",
        "interim_dir = os.path.join(OUT_ROOT, \"interim\")\n",
        "ensure_dir(raw_dir); ensure_dir(interim_dir)\n",
        "\n",
        "# build the target time window \n",
        "# approx_age_hours clusters around ~60–90 hours rather than exactly 72\n",
        "to_dt = lambda s: datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
        "_start = to_dt(iso_day_bounds_utc(DAYS_AGO)[0]) - timedelta(hours=WIDEN_HOURS)\n",
        "_end   = to_dt(iso_day_bounds_utc(DAYS_AGO)[1]) + timedelta(hours=WIDEN_HOURS)\n",
        "published_after = _start.isoformat().replace(\"+00:00\",\"Z\")\n",
        "published_before = _end.isoformat().replace(\"+00:00\",\"Z\")\n",
        "print(\"Target UTC window:\", published_after, \"→\", published_before)\n",
        "\n",
        "# Save categories reference\n",
        "try:\n",
        "    _ = fetch_categories(API_KEY, REGION_CODE, raw_dir)\n",
        "except Exception as e:\n",
        "    print(\"Warning fetching categories:\", e)\n",
        "\n",
        "# Search sweep (one big run; all categories together)\n",
        "video_ids = search_all_categories_day(\n",
        "    api_key=API_KEY,\n",
        "    region_code=REGION_CODE,\n",
        "    relevance_language=RELEVANCE_LANGUAGE,\n",
        "    published_after=published_after,\n",
        "    published_before=published_before,\n",
        "    search_units_budget=SEARCH_UNITS,\n",
        "    raw_dir=raw_dir\n",
        ")\n",
        "print(f\"Found {len(video_ids)} video IDs (after de-dupe)\")\n",
        "if not video_ids:\n",
        "    raise SystemExit(\"No videos found. Consider increasing SEARCH_UNITS or adjusting DAYS_AGO.\")\n",
        "\n",
        "# Enrich each ID with details/stats (videos.list)\n",
        "vids_df = videos_details(API_KEY, video_ids, raw_dir)\n",
        "print(\"videos.list returned rows:\", len(vids_df))\n",
        "if vids_df.empty:\n",
        "    raise SystemExit(\"No details returned by videos.list.\")\n",
        "\n",
        "# 3) Filter out Shorts\n",
        "vids_df[\"duration_sec\"] = pd.to_numeric(vids_df[\"duration_sec\"], errors=\"coerce\")\n",
        "# Modification to 180 seconds from previous 60 seconds due to discovering newer time limit for shorts. Additional filtering is carried out in combine_and_dedupe.py\n",
        "vids_df[\"is_shorts\"] = (vids_df[\"duration_sec\"] <= 180)\n",
        "vids_df = vids_df[~vids_df[\"is_shorts\"]].copy()\n",
        "\n",
        "# 4) Snapshot timing & ~72h renames,recording when looked (snapshot_at_utc) and how old each video was then.\n",
        "snapshot_at = datetime.now(timezone.utc)\n",
        "vids_df[\"publishedAt_utc\"] = pd.to_datetime(vids_df[\"publishedAt\"], utc=True, errors=\"coerce\")\n",
        "vids_df[\"snapshot_at_utc\"] = snapshot_at.isoformat().replace(\"+00:00\",\"Z\")\n",
        "vids_df[\"approx_age_hours\"] = ((snapshot_at - vids_df[\"publishedAt_utc\"]).dt.total_seconds() / 3600.0).round(2)\n",
        "vids_df.rename(columns={\n",
        "    \"viewCount\": \"views_72h\",\n",
        "    \"likeCount\": \"likes_72h\",\n",
        "    \"commentCount\": \"comments_72h\",\n",
        "}, inplace=True)\n",
        "\n",
        "# 5) Channel stats\n",
        "try:\n",
        "    ch_df = channels_stats(API_KEY, vids_df[\"channelId\"].dropna().tolist(), raw_dir)\n",
        "    if ch_df is not None and not ch_df.empty:\n",
        "        vids_df = vids_df.merge(ch_df, on=\"channelId\", how=\"left\")\n",
        "except Exception as e:\n",
        "    print(\"Warning fetching channel stats:\", e)\n",
        "\n",
        "# quick flag to analyze how often engagement stats are missing\n",
        "vids_df[\"has_missing_stats\"] = vids_df[[\"likes_72h\",\"comments_72h\"]].isna().any(axis=1)\n",
        "\n",
        "# 6) Select & save tidy columns\n",
        "cols = [\n",
        "    \"videoId\",\"channelId\",\"categoryId\",\n",
        "    \"publishedAt_utc\",\"snapshot_at_utc\",\"approx_age_hours\",\n",
        "    \"title\",\"description\",\"tags_str\",\n",
        "    \"duration_sec\",\"is_shorts\",\"has_missing_stats\",\n",
        "    \"views_72h\",\"likes_72h\",\"comments_72h\",\n",
        "    \"channel_subscriberCount\",\"channel_viewCount\",\"channel_videoCount\",\"channel_hiddenSubscriberCount\",\n",
        "]\n",
        "tidy = vids_df[cols].copy()\n",
        "\n",
        "run_stamp = ts_now_utc()\n",
        "out_parquet = os.path.join(interim_dir, f\"labels_input_{run_stamp}.parquet\")\n",
        "tidy.to_parquet(out_parquet, index=False)\n",
        "if SAVE_CSV:\n",
        "    out_csv = os.path.join(interim_dir, f\"labels_input_{run_stamp}.csv\")\n",
        "    tidy.to_csv(out_csv, index=False)\n",
        "\n",
        "#text confirmation that files have been saved\n",
        "print(\"\\nSaved:\")\n",
        "print(\" \", out_parquet)\n",
        "if SAVE_CSV:\n",
        "    print(\" \", out_csv)\n",
        "print(f\"Rows: {len(tidy)} | Unique videos: {tidy['videoId'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a54e5aec",
      "metadata": {},
      "source": [
        "### API Quota Reference (rough)\n",
        "\n",
        "- `search.list`: ~100 units **per page**\n",
        "- `videos.list`: 1 unit per call (up to 50 ids)\n",
        "- `channels.list`: 1 unit per call (up to 50 ids)\n",
        "\n",
        "A `SEARCH_UNITS=9000` budget ≈ 90 pages of search results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
